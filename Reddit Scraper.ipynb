{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import praw\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file containing authentication info\n",
    "with open('authentication.json', 'r') as openfile: \n",
    "    auth = json.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as /u/cam_man_can\n"
     ]
    }
   ],
   "source": [
    "reddit_obj = praw.Reddit(client_id=auth[\"client_id\"], \n",
    "                         client_secret=auth[\"client_secret\"],\n",
    "                         user_agent=auth[\"user_agent\"],\n",
    "                         username=auth[\"username\"],\n",
    "                         password=auth[\"password\"])\n",
    "\n",
    "print('Authenticated as /u/{}'.format(reddit_obj.user.me()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract posts, save them to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"reddit_data/\"\n",
    "subreddit_name = 'NeutralPolitics'\n",
    "posts_file_path = directory + subreddit_name + '_posts.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_posts(reddit, subreddit, posts_file):\n",
    "    \"\"\"\n",
    "    Scrapes posts from specified subreddit, logs post information into csv file.\n",
    "    \n",
    "    Currently configured to scrape \"top\" posts, with date range set to \"all\" (all time).\n",
    "    \n",
    "    Reddit API usually limits to 1000 posts. \n",
    "    \n",
    "    Submission attributes: https://praw.readthedocs.io/en/latest/code_overview/models/submission.html\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(os.path.dirname(posts_file)):\n",
    "        os.makedirs(os.path.dirname(posts_file))\n",
    "\n",
    "    with open(posts_file, \"a\",  encoding=\"utf-8\") as outfile:\n",
    "        for submission in reddit.subreddit(subreddit).top('all', limit=1000):\n",
    "            print(\"\\r \\n\", submission.title, end='')\n",
    "            data = [\n",
    "                submission.title,\n",
    "                submission.author,\n",
    "                submission.created_utc,\n",
    "                submission.score,\n",
    "                submission.domain,\n",
    "                \"%r\" % submission.selftext,\n",
    "                submission.id,\n",
    "                submission.upvote_ratio,\n",
    "                submission.num_comments\n",
    "            ]\n",
    "            writer = csv.writer(outfile)\n",
    "            writer.writerow(data)\n",
    "            # time.sleep(1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_posts(reddit_obj, subreddit_name, posts_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract comments from scraped posts, save to another .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_file_path = directory + subreddit_name + '_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(reddit, posts_file, comments_file):\n",
    "    \"\"\"\n",
    "    Takes csv file containing post information, and scrapes the comments for each post. \n",
    "    \n",
    "    The amount of comments to scrape is controlled by the paramaters of submission.comments.replace_more().     \n",
    "    \n",
    "    Comment scraping tutorial: https://praw.readthedocs.io/en/latest/tutorials/comments.html\n",
    "    Comment attributes: https://praw.readthedocs.io/en/latest/code_overview/models/comment.html#praw.models.Comment\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(os.path.dirname(posts_file)):\n",
    "        os.makedirs(os.path.dirname(posts_file))\n",
    "        \n",
    "    print(\"Fetching comments ...\")\n",
    "    with open(posts_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        row_counter = 0\n",
    "\n",
    "        for row in reader:\n",
    "            \n",
    "            if (len(row) == 0):\n",
    "                continue\n",
    "            \n",
    "            post_id = str(row[6])\n",
    "            row_counter+=1\n",
    "\n",
    "            if post_id == \"6am00f\":\n",
    "                continue\n",
    "\n",
    "            submission = reddit.submission(id=post_id)\n",
    "            time.sleep(1)\n",
    "            print(\"\\r post count:%s\" % (row_counter), end='')\n",
    "            submission.comments.replace_more(limit=None) \n",
    "\n",
    "            for comment in submission.comments.list():                    \n",
    "                if isinstance(comment, praw.models.MoreComments): \n",
    "                    continue\n",
    "                \n",
    "                comment_str = comment.body\n",
    "                comment_author = comment.author\n",
    "                comment_score = comment.score\n",
    "                comment_created_utc = comment.created_utc\n",
    "                \n",
    "                if comment_str == \"[deleted]\" or comment_str == \"[removed]\":\n",
    "                    continue\n",
    "                \n",
    "                with open(comments_file, \"a\",  encoding=\"utf-8\") as outfile:\n",
    "                    writer = csv.writer(outfile)\n",
    "                    writer.writerow([\"%r\" % comment_str, comment_author, comment_score, comment_created_utc, post_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching comments ...\n",
      " post count:474"
     ]
    }
   ],
   "source": [
    "get_comments(reddit_obj, posts_file_path, comments_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

